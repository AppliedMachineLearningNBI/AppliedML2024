{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flight Passenger Predictions\n",
    "\n",
    "This is a simple example of using an LSTM for predicting the number of monthly flight passengers. The (real) data spans the years 1949-1960, and thus it contains 12x12=144 entries with the number of passengers (in thousands). Your job is to predict the next point(s) in the sequence.\n",
    "\n",
    "There are several ML packages, which have an LSTM implementation, and probably most widely used are:\n",
    "* Keras Tensorflow (see e.g. https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n",
    "* PyTorch (see e.g. https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html)\n",
    "\n",
    "***\n",
    "\n",
    "Author: Troels Petersen<br>\n",
    "Date: 5th of May 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM for international airline passengers problem with regression framing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert an array of values into a dataset matrix:\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed for reproducibility:\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the dataset:\n",
    "dataframe = read_csv('airline-passengers.csv', usecols=[1], engine='python')\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "\n",
    "# Normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "# Reshape into X = t and Y = t + 1, and include the last 5 entries in prediction:\n",
    "look_back = 5\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.shape, trainY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an `x` such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX[15, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to predict the next value of the sequence, we we have stored as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to PyTorch tensors:\n",
    "trainX = torch.tensor(trainX, dtype=torch.float)\n",
    "trainY = torch.tensor(trainY, dtype=torch.float)\n",
    "testX = torch.tensor(testX, dtype=torch.float)\n",
    "testY = torch.tensor(testY, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make an LSTM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_size=7):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)  # Run LSTM and store the hidden layer outputs\n",
    "        x = x[:, -1, :]  # take the last hidden layer\n",
    "        x = self.linear(x) # a normal dense layer\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(net.parameters(), lr=5e-3)\n",
    "progress_bar = tqdm(range(5000))\n",
    "for epoch in progress_bar:\n",
    "    prediction = net(trainX)\n",
    "    loss = torch.sum((prediction.flatten() - trainY.flatten())**2)\n",
    "    progress_bar.set_description(f'Loss = {float(loss):6.4f}')\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the result/performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions:\n",
    "with torch.no_grad():\n",
    "    trainPredict = net(trainX).numpy()\n",
    "    testPredict = net(testX).numpy()\n",
    "\n",
    "# Invert predictions:\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY_inv = scaler.inverse_transform([trainY.numpy()])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY_inv = scaler.inverse_transform([testY.numpy()])\n",
    "\n",
    "# Calculate root mean squared error:\n",
    "trainScore = math.sqrt(mean_squared_error(trainY_inv[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY_inv[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "# Shift train predictions for plotting:\n",
    "trainPredictPlot = np.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "# Shift test predictions for plotting:\n",
    "testPredictPlot = np.empty_like(dataset)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "\n",
    "# Plot baseline and predictions:\n",
    "plt.plot(scaler.inverse_transform(dataset), label=\"Data\")\n",
    "plt.plot(trainPredictPlot, label=\"Training predictions\")\n",
    "plt.plot(testPredictPlot, label=\"Testing predictions\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises:\n",
    "\n",
    "1. As always, look through the code and convince yourself, that you understand each step.\n",
    "2. Discuss with your peers, what the input and output to the LSTM looks like, and potentially more advanced datasets.\n",
    "3. Can you draw the LSTM architecture? And how many parameters does the current model use?\n",
    "4. The result suggests over-fitting! Play with the LSTM-architechture/training procedure to try to fix this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning points:\n",
    "\n",
    "Generally, you should realise, that **time series data** can be analysed with **RNNs, LSTMs, and GRUs**. GRUs are particularly suited for numerical data (as opposted to natural language data).\n",
    "\n",
    "a. It is - as always - important to inspect the data, and certainly to plot it.<br>\n",
    "b. One should carefully consider how far back in time input data should be included.<br>\n",
    "c. One should carefully consider how far into the future the predictions are made.<br>\n",
    "\n",
    "PS. As this is a purely numerical problem, one should probably have used a GRU instead (simpler, better).\n",
    "    Note that GRUs can be bi-directional, in case the problem involves a series of data with no obvious direction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
