{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIV-Inhibiting Molecule Classification using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roadmap\n",
    "\n",
    "1. Introduce the problem and the dataset\n",
    "2. Import libraries\n",
    "3. Load the data\n",
    "4. Visualize the data\n",
    "5. Visualize the metrics that need to be achieved\n",
    "6. Try logistic regression\n",
    "7. Try a simple neural network\n",
    "8. Try a simple GCN\n",
    "9. Try a simple GAT\n",
    "10. Try playing with convolution and aggregation styles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a conda env already set up\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This next install depends on whether you have a cpu or gpu. If you have a gpu, you can install the gpu version of torch_geometric:\n",
    "!pip install torch_scatter -f https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
    "# If you have a cpu, comment out the above line and run the following command:\n",
    "#!pip install torch_scatter -f https://data.pyg.org/whl/torch-2.3.0+cpu.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nodes in the graph have features given by `graph.x`:\n",
    "- atomic_num: the atomic number of the atom\n",
    "- chirality: the chirality of the atom\n",
    "- degree: the degree of the atom\n",
    "- formal_charge: the formal charge of the atom\n",
    "- num_h: the number of hydrogens of the atom\n",
    "- num_rad_e: the number of radical electrons of the atom\n",
    "- hybridization: the hybridization of the atom\n",
    "- is_aromatic: whether the atom is aromatic\n",
    "- is_in_ring: whether the atom is in a ring\n",
    "\n",
    "The edges in the graph have features given by `graph.edge_attr`: \n",
    "- bond_type: the type of the bond\n",
    "- bond_stereo: the stereo of the bond\n",
    "- is_conjugated: whether the bond is conjugated\n",
    "\n",
    "The graph has just one \"feature\", which is a binary label indicating whether the molecule is an HIV-inhibitor or not:\n",
    "- `graph.y`: whether the molecule is an HIV-inhibitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.graphproppred import PygGraphPropPredDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "dataset = PygGraphPropPredDataset(name = \"ogbg-molhiv\") \n",
    "\n",
    "split_idx = dataset.get_idx_split() \n",
    "train_pyg_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=32, shuffle=True)\n",
    "valid_pyg_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=32, shuffle=False)\n",
    "test_pyg_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_graph = valid_pyg_loader.dataset[0]\n",
    "print(example_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Function to visualize graph based on target value\n",
    "def visualize_graph_with_target(loader, target_value):\n",
    "    for graph in loader.dataset:\n",
    "        if graph.y.item() == target_value:\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            atomic_number = graph.x[:, 0].numpy()\n",
    "            graph_networkx = to_networkx(graph.clone())\n",
    "            pos = nx.spring_layout(graph_networkx)\n",
    "            nx.draw_networkx(graph_networkx, pos, labels=dict(zip(range(len(atomic_number)), atomic_number)), node_size=300, node_color=atomic_number, cmap=\"tab20\")\n",
    "            plt.title(f\"Graph with target {graph.y.item()}\")\n",
    "            plt.show()\n",
    "            break\n",
    "\n",
    "# Visualize graph with target 0\n",
    "visualize_graph_with_target(valid_pyg_loader, 0)\n",
    "\n",
    "# Visualize graph with target 1\n",
    "visualize_graph_with_target(valid_pyg_loader, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms of graph sizes for true and false graphs\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def plot_histograms_of_graph_sizes(loader):\n",
    "    true_sizes = []\n",
    "    false_sizes = []\n",
    "    for graph in loader.dataset:\n",
    "        if graph.y.item() == 1:\n",
    "            true_sizes.append(graph.num_nodes)\n",
    "        else:\n",
    "            false_sizes.append(graph.num_nodes)\n",
    "    \n",
    "    # Create a DataFrame to facilitate plotting with seaborn\n",
    "    data_true = pd.DataFrame({'Size': true_sizes, 'Target': 'True'})\n",
    "    data_false = pd.DataFrame({'Size': false_sizes, 'Target': 'False'})\n",
    "    data = pd.concat([data_true, data_false])\n",
    "    \n",
    "    # Plotting the histograms\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data, x='Size', hue='Target', element='step', kde=True, palette=['green', 'red'])\n",
    "    plt.title('Histogram of Graph Sizes for True and False Targets')\n",
    "    plt.xlabel('Graph Size')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "plot_histograms_of_graph_sizes(valid_pyg_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the data is VERY imbalanced - most molecules are not useful for fighting HIV (as you might expect!). So we will need to be careful about how we evaluate our models. We can't simply use accuracy, as a model that always predicts \"not an HIV-inhibitor\" would be right 99% of the time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also histogram each of the node features, so we know what scales they are:\n",
    "def plot_histograms_of_node_features(loader):\n",
    "    feature_names = ['atomic_num', 'chirality', 'degree', 'formal_charge', 'num_h', 'num_rad_e', 'hybridization', 'is_aromatic', 'is_in_ring']\n",
    "    num_features = len(feature_names)\n",
    "    \n",
    "    # Create a 3x3 subplot grid\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    axes = axes.flatten()  # Flatten the 2D array of axes to 1D for easier iteration\n",
    "    \n",
    "    for i in range(num_features):\n",
    "        all_features = []\n",
    "        for graph in loader.dataset:\n",
    "            all_features.extend(graph.x[:, i].numpy())\n",
    "        \n",
    "        # Plot on the ith subplot\n",
    "        sns.histplot(all_features, element='step', ax=axes[i])\n",
    "        axes[i].set_title(f'Histogram of {feature_names[i]}')\n",
    "        axes[i].set_xlabel(f'{feature_names[i]} Value')\n",
    "        axes[i].set_ylabel('Count')\n",
    "        axes[i].set_yscale('log')  # Set y-axis scale to logarithmic\n",
    "    \n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_histograms_of_node_features(valid_pyg_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the data is all of a similar scale, so we don't need to be too worried about normalizing. Maybe we could take the log of the atomic number, and add that as a feature. But that's about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the absolute most naive thing: Logistic regression. This will give us a baseline to compare against.\n",
    "The first question is what is the input to our logistic regression. It's non-obvious, since we have two problems:\n",
    "1. The input features are nodes in no obvious or particular order\n",
    "2. The list of nodes is different from one graph to the next\n",
    "\n",
    "So to make our first naive benchmark as simple as possible, let's take some simplifications to the dataset:\n",
    "1. Let's sort the nodes from largest to smallest atomic number\n",
    "2. Let's take the first 10 sorted nodes in each graph, and pad with zeros if there are fewer than 20 nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort the nodes by atomic number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "# Function to preprocess and flatten graph data\n",
    "def preprocess_data(loader):\n",
    "    X, y = [], []\n",
    "    for data in loader.dataset:\n",
    "        # Sort nodes by atomic number in descending order\n",
    "        node_features = data.x\n",
    "        sorted_nodes = node_features[node_features[:, 0].argsort(descending=True)]\n",
    "        \n",
    "        # Add log of the first feature (atomic number) as a new feature\n",
    "        log_feature = np.log(sorted_nodes[:, 0] + 1).reshape(-1, 1)  # Adding 1 to avoid log(0)\n",
    "        sorted_nodes = np.hstack([sorted_nodes, log_feature])\n",
    "        \n",
    "        # Select the first N nodes\n",
    "        num_nodes = 20\n",
    "\n",
    "        # Select the first 20 nodes, pad if necessary\n",
    "        if len(sorted_nodes) < num_nodes:\n",
    "            padding = np.zeros((num_nodes - len(sorted_nodes), sorted_nodes.shape[1]))\n",
    "            sorted_nodes = np.vstack([sorted_nodes, padding])\n",
    "        elif len(sorted_nodes) > num_nodes:\n",
    "            sorted_nodes = sorted_nodes[:num_nodes]\n",
    "        \n",
    "        # Flatten the node features to create a single feature vector\n",
    "        flat_features = sorted_nodes.flatten()\n",
    "        X.append(flat_features)\n",
    "        y.append(data.y.item())\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "X_train, y_train = preprocess_data(train_pyg_loader)\n",
    "X_valid, y_valid = preprocess_data(valid_pyg_loader)\n",
    "X_test, y_test = preprocess_data(test_pyg_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_valid = model.predict(X_valid)\n",
    "y_pred_test = model.predict(X_test)\n",
    "valid_accuracy = accuracy_score(y_valid, y_pred_valid)\n",
    "valid_auc = roc_auc_score(y_valid, model.predict_proba(X_valid)[:, 1])\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "test_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(f\"Validation Accuracy: {valid_accuracy}\")\n",
    "print(f\"Validation AUC: {valid_auc}\")\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test AUC: {test_auc}\")\n",
    "\n",
    "# Also get the ROC AUC for a random classifier\n",
    "random_preds = np.random.rand(len(y_test))\n",
    "random_auc = roc_auc_score(y_test, random_preds)\n",
    "\n",
    "fpr_valid, tpr_valid, _ = roc_curve(y_valid, model.predict_proba(X_valid)[:, 1])\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, model.predict_proba(X_test)[:, 1])\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_valid, tpr_valid, label='Validation')\n",
    "plt.plot(fpr_test, tpr_test, label='Test')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='black')\n",
    "plt.text(0.7, 0.3, f'Valid AUC: {valid_auc:.2f}', fontsize=12)\n",
    "plt.text(0.7, 0.2, f'Test AUC: {test_auc:.2f}', fontsize=12)\n",
    "plt.text(0.7, 0.1, f'Random AUC: {random_auc:.2f}', fontsize=12)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Logistic Regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we're better than a coin flip. Not bad for 3 seconds of work. But we have a long way to go if we want to get onto the OGB Leaderboard for this benchmark:\n",
    "![OGB Leaderboard](img/mol_hiv_leaderboard.png)\n",
    "\n",
    "So let's try something more sophisticated...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a BDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Train Gradient Boosting model\n",
    "bdt_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "bdt_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model using AUC\n",
    "y_prob_valid_bdt = bdt_model.predict_proba(X_valid)[:, 1]  # Get probabilities for the positive class\n",
    "y_prob_test_bdt = bdt_model.predict_proba(X_test)[:, 1]    # Get probabilities for the positive class\n",
    "\n",
    "valid_auc_bdt = roc_auc_score(y_valid, y_prob_valid_bdt)\n",
    "test_auc_bdt = roc_auc_score(y_test, y_prob_test_bdt)\n",
    "\n",
    "print(f\"Validation AUC (BDT): {valid_auc_bdt}\")\n",
    "print(f\"Test AUC (BDT): {test_auc_bdt}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr_valid_bdt, tpr_valid_bdt, _ = roc_curve(y_valid, y_prob_valid_bdt)\n",
    "fpr_test_bdt, tpr_test_bdt, _ = roc_curve(y_test, y_prob_test_bdt)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_valid_bdt, tpr_valid_bdt, label='Validation (BDT)')\n",
    "plt.plot(fpr_test_bdt, tpr_test_bdt, label='Test (BDT)')\n",
    "plt.plot(fpr_valid, tpr_valid, label='Validation (LR)')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='black')\n",
    "plt.text(0.7, 0.3, f'Validation AUC (BDT): {valid_auc_bdt:.2f}', fontsize=12)\n",
    "plt.text(0.7, 0.2, f'Test AUC (BDT): {test_auc_bdt:.2f}', fontsize=12)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creeping up... but still a long way to go to an AUC of 0.84. Let's try a simple neural network next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a simple neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this simple neural network, let's switch back to pytorch. In particular, let's use pytorch lightning to avoid boilerplate code. (If you haven't used pytorch or pytorch lightning before, you can think of pytorch = tensorboard, and pytorch lightning = keras). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as lit\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "from lightning.pytorch import Trainer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from lightning.pytorch.loggers import CSVLogger  # Import CSVLogger\n",
    "import numpy as np\n",
    "\n",
    "def prepare_data_loaders(X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size=32):\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "    valid_dataset = TensorDataset(torch.FloatTensor(X_valid), torch.FloatTensor(y_valid))\n",
    "    test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "def make_mlp(\n",
    "    input_size,\n",
    "    sizes,\n",
    "    hidden_activation=\"ReLU\",\n",
    "    output_activation=None,\n",
    "    layer_norm=False,\n",
    "):\n",
    "    \"\"\"Construct an MLP with specified fully-connected layers.\"\"\"\n",
    "    hidden_activation = getattr(nn, hidden_activation)()\n",
    "    output_activation = getattr(nn, output_activation)() if output_activation else None\n",
    "    layers = []\n",
    "    sizes = [input_size] + sizes\n",
    "\n",
    "    # Add hidden layers\n",
    "    for i in range(len(sizes) - 2):  # Change the range to stop before the last hidden layer\n",
    "        layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "        if layer_norm:\n",
    "            layers.append(nn.LayerNorm(sizes[i + 1], elementwise_affine=False))\n",
    "        layers.append(hidden_activation)\n",
    "\n",
    "    # Add final layer\n",
    "    layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
    "    if output_activation:\n",
    "        layers.append(output_activation)\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class BaseLitModel(lit.LightningModule):\n",
    "    def __init__(self, lr=1e-3, weight_decay=0, lr_decay=0.95):\n",
    "        super().__init__()\n",
    "\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.val_preds = []\n",
    "        self.val_targets = []\n",
    "        self.test_preds = []\n",
    "        self.test_targets = []\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.lr_decay = lr_decay\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_hat, y = self.apply_model(batch)\n",
    "        # Check if y needs to be unsqueezed\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.unsqueeze(1)\n",
    "        loss = self.criterion(y_hat, y.float())\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)  # Log on progress bar\n",
    "        self.log('learning_rate', self.trainer.optimizers[0].param_groups[0]['lr'], on_step=False, on_epoch=True, prog_bar=True)  # Log current learning rate\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_hat, y = self.apply_model(batch)\n",
    "        # Check if y needs to be unsqueezed\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.unsqueeze(1)\n",
    "        loss = self.criterion(y_hat, y.float())\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)  # Log on progress bar\n",
    "        self.val_preds.extend(y_hat.view(-1).detach().cpu().numpy())\n",
    "        self.val_targets.extend(y.view(-1).detach().cpu().numpy())\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        y_hat, y = self.apply_model(batch)\n",
    "        # Check if y needs to be unsqueezed\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.unsqueeze(1)\n",
    "        loss = self.criterion(y_hat, y.float())\n",
    "        self.log('test_loss', loss, on_epoch=True, prog_bar=True)  # Log on progress bar\n",
    "        self.test_preds.extend(y_hat.view(-1).detach().cpu().numpy())\n",
    "        self.test_targets.extend(y.view(-1).detach().cpu().numpy())\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        lr_scheduler = {\n",
    "            'scheduler': torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: self.lr_decay ** epoch),\n",
    "            'name': 'lambda_decay'\n",
    "        }\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "class AUCCallback(Callback):\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        val_auc = roc_auc_score(pl_module.val_targets, pl_module.val_preds)\n",
    "        pl_module.log(\"val_auc\", val_auc, prog_bar=True)  # Log on progress bar\n",
    "        pl_module.val_preds = []\n",
    "        pl_module.val_targets = []\n",
    "\n",
    "    def on_test_epoch_end(self, trainer, pl_module):\n",
    "        test_auc = roc_auc_score(pl_module.test_targets, pl_module.test_preds)\n",
    "        pl_module.log(\"test_auc\", test_auc, prog_bar=True)  # Log on progress bar\n",
    "        pl_module.test_preds = []\n",
    "        pl_module.test_targets = []\n",
    "\n",
    "# Prepare data loaders\n",
    "def prepare_data_loaders(X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size=32):\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "    valid_dataset = TensorDataset(torch.FloatTensor(X_valid), torch.FloatTensor(y_valid))\n",
    "    test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(BaseLitModel):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleMLP, self).__init__(lr=1e-3, weight_decay=0, lr_decay=0.90)\n",
    "        self.model = make_mlp(input_size, \n",
    "                              [hidden_size]*3 + [output_size],\n",
    "                              hidden_activation=\"ReLU\", \n",
    "                              layer_norm=True,\n",
    "                              output_activation=\"Sigmoid\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def apply_model(self, batch):\n",
    "        x, y = batch\n",
    "        return self(x), y\n",
    "        \n",
    "# Data loaders\n",
    "train_loader, valid_loader, test_loader = prepare_data_loaders(X_train, y_train, X_valid, y_valid, X_test, y_test)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = train_loader.dataset[0][0].shape[0]  # Adjust input size for the new feature\n",
    "hidden_size = 64  # You can tune this parameter\n",
    "output_size = 1\n",
    "model = SimpleMLP(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the model\n",
    "auc_callback = AUCCallback()\n",
    "csv_logger = CSVLogger('lightning_logs', name='mlp_model')  # Initialize CSVLogger\n",
    "trainer = Trainer(max_epochs=10, accelerator=\"gpu\", callbacks=[auc_callback], logger=csv_logger)  # Add logger to Trainer\n",
    "trainer.fit(model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def find_latest_directory(base_path):\n",
    "    list_of_files = glob.glob(f'{base_path}/version_*')\n",
    "    latest_dir = max(list_of_files, key=os.path.getmtime)\n",
    "    return latest_dir\n",
    "\n",
    "def plot_metrics(directory, model_name):\n",
    "    metrics_file = os.path.join(directory, 'metrics.csv')\n",
    "    metrics_df = pd.read_csv(metrics_file)\n",
    "    train_loss_df = metrics_df[metrics_df['train_loss'].notna()]\n",
    "    val_loss_df = metrics_df[metrics_df['val_loss'].notna()]\n",
    "    val_auc_df = metrics_df[metrics_df['val_auc'].notna()]\n",
    "    learning_rate_df = metrics_df[metrics_df['learning_rate'].notna()]  # Extract learning rate data\n",
    "\n",
    "    plt.figure(figsize=(16, 4))  # Adjusted figure size to accommodate new subplot\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.plot(train_loss_df['step'], train_loss_df['train_loss'], label='Training Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xscale('log')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.plot(val_loss_df['epoch'], val_loss_df['val_loss'], label='Validation Loss')\n",
    "    plt.title('Validation Loss Over Epochs')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xscale('log')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.plot(val_auc_df['epoch'], val_auc_df['val_auc'], label='Validation AUC')\n",
    "    plt.title('Validation AUC Over Epochs')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.xscale('log')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.plot(learning_rate_df['epoch'], learning_rate_df['learning_rate'], label='Learning Rate')\n",
    "    plt.title('Learning Rate Over Epochs')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.xscale('log')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'lightning_logs/mlp_model'\n",
    "latest_dir = find_latest_directory(base_path)\n",
    "plot_metrics(latest_dir, 'MLP Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we're struggling to beat the BDT - it's just too good at consuming tabular data. But we have a secret weapon: the graph structure! Let's try a simple graph convolution network. We will use the industry-standard PyG (\"Pytorch Geometric\") library for this, with its built-in GCN implementation. But first, let's try to code up a graph neural network from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_scatter import scatter_mean, scatter_add\n",
    "from torch import nn\n",
    "\n",
    "class InteractionGNN(BaseLitModel):\n",
    "    def __init__(self, hparams):\n",
    "        super(InteractionGNN, self).__init__(lr=hparams['lr'], weight_decay=hparams['weight_decay'], lr_decay=hparams['lr_decay'])\n",
    "\n",
    "        # Node and Edge Networks\n",
    "        self.input_node_network = make_mlp(\n",
    "            input_size=hparams['input_size'],\n",
    "            sizes=[hparams['hidden_size']] * 2,\n",
    "            layer_norm=True,\n",
    "            hidden_activation=\"ReLU\"\n",
    "        )\n",
    "        self.node_network = make_mlp(\n",
    "            input_size=hparams['hidden_size'],\n",
    "            sizes=[hparams['hidden_size']] * 2,\n",
    "            layer_norm=True,\n",
    "            hidden_activation=\"ReLU\"\n",
    "        )\n",
    "        self.edge_network = make_mlp(\n",
    "            input_size=2 * hparams['hidden_size'],\n",
    "            sizes=[hparams['hidden_size']] * 2,\n",
    "            layer_norm=True,\n",
    "            hidden_activation=\"ReLU\"\n",
    "        )\n",
    "        self.output_network = make_mlp(\n",
    "            input_size=hparams['hidden_size'],\n",
    "            sizes=[hparams['output_size']],\n",
    "            layer_norm=True,\n",
    "            hidden_activation=\"ReLU\",\n",
    "            output_activation=\"Sigmoid\"\n",
    "        )\n",
    "        self.num_graph_iters = hparams['num_graph_iters']\n",
    "\n",
    "        self.aggregation = scatter_add\n",
    "\n",
    "    def forward(self, graph):\n",
    "        start, end = edge_index\n",
    "        x, edge_index = graph.x.float(), graph.edge_index\n",
    "        x = self.input_node_network(x)\n",
    "\n",
    "        for _ in range(self.num_graph_iters):\n",
    "            # Save the input to the iteration for the skip connection\n",
    "            x_skip = x\n",
    "\n",
    "            # Edge features update\n",
    "            e = self.message_passing(x, start, end)\n",
    "\n",
    "            # Message passing\n",
    "            x = self.aggregation(e, end, dim=0, dim_size=x.size(0))  # Aggregating messages to target nodes\n",
    "\n",
    "            x = scatter_add(e, end)\n",
    "\n",
    "            # Node features update\n",
    "            x = self.node_network(x)\n",
    "\n",
    "            # Adding skip connection\n",
    "            x = x + x_skip\n",
    "\n",
    "        # Aggregate to graph level\n",
    "        x = self.aggregation(x, graph.batch, dim=0)\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.output_network(x)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def message_passing(self, x, start, end):\n",
    "        \n",
    "        edge_features = torch.cat([x[start], x[end]], dim=1)\n",
    "        e = self.edge_network(edge_features)\n",
    "        return e\n",
    "\n",
    "    def apply_model(self, batch):\n",
    "        return self(batch), batch.y\n",
    "\n",
    "# Example usage\n",
    "hparams = {\n",
    "    'input_size': train_pyg_loader.dataset[0].x.shape[1],\n",
    "    'hidden_size': 64,\n",
    "    'output_size': 1,\n",
    "    'lr': 3e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'lr_decay': 0.97,\n",
    "    'num_graph_iters': 3  # Number of graph iterations\n",
    "}\n",
    "model = InteractionGNN(hparams)\n",
    "\n",
    "# Training setup remains the same\n",
    "auc_callback = AUCCallback()\n",
    "csv_logger = CSVLogger('lightning_logs', name='interaction_gnn_model')\n",
    "trainer = Trainer(max_epochs=50, accelerator=\"gpu\", callbacks=[auc_callback], logger=csv_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_pyg_loader, valid_pyg_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'lightning_logs/interaction_gnn_model'\n",
    "latest_dir = find_latest_directory(base_path)\n",
    "plot_metrics(latest_dir, 'Interaction GNN Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "trainer.test(model, test_pyg_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.graphproppred import Evaluator\n",
    "\n",
    "evaluator = Evaluator(name = 'ogbg-molhiv')\n",
    "\n",
    "y_true = torch.cat([graph.y for graph in valid_pyg_loader]).numpy()\n",
    "with torch.no_grad():\n",
    "    y_pred = torch.cat([model(graph) for graph in valid_pyg_loader]).numpy()\n",
    "input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
    "result_dict = evaluator.eval(input_dict)\n",
    "print(\"VALIDATION RESULTS\", result_dict)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.graphproppred import Evaluator\n",
    "\n",
    "evaluator = Evaluator(name = 'ogbg-molhiv')\n",
    "\n",
    "y_true = torch.cat([graph.y for graph in test_pyg_loader]).numpy()\n",
    "with torch.no_grad():\n",
    "    y_pred = torch.cat([model(graph) for graph in test_pyg_loader]).numpy()\n",
    "input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
    "result_dict = evaluator.eval(input_dict)\n",
    "print(\"TEST RESULTS\", result_dict)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try your own Torch Geometric model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with convolution and aggregation styles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tweak the convolution and aggregation styles to see if we can improve the performance of our model. E.g. we can try scatter max, scatter std, combine scatter operations, or we can try the MANY different convolutions available in PyG. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can also try out-of-the-box convolutions from Pytorch Geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a simple graph convolution network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as lit\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "from lightning.pytorch import Trainer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GCNConv, aggr\n",
    "from lightning.pytorch.loggers import CSVLogger  # Import CSVLogger\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "class GCN(BaseLitModel):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GCN, self).__init__(lr=1e-3, weight_decay=0, lr_decay=0.90)\n",
    "        self.node_network_1 = make_mlp(input_size, [hidden_size]*2, layer_norm=True, hidden_activation=\"ReLU\")\n",
    "        self.conv = GCNConv(hidden_size, hidden_size)\n",
    "        self.node_network_2 = make_mlp(hidden_size, [hidden_size]*2, layer_norm=True, hidden_activation=\"ReLU\")\n",
    "        self.output_network = make_mlp(hidden_size, [output_size], layer_norm=True, hidden_activation=\"ReLU\", output_activation=\"Sigmoid\")\n",
    "        self.aggregation = aggr.MeanAggregation()\n",
    "\n",
    "    def forward(self, graph):\n",
    "        x = self.node_network_1(graph.x.float())\n",
    "        x = self.conv(x, graph.edge_index)\n",
    "        x = self.node_network_2(x)\n",
    "        x = self.aggregation(x, graph.batch)\n",
    "        return self.output_network(x)\n",
    "\n",
    "    def apply_model(self, batch):\n",
    "        return self(batch), batch.y\n",
    "\n",
    "# Initialize the model\n",
    "input_size = train_pyg_loader.dataset[0].x.shape[1]\n",
    "hidden_size = 64  # You can tune this parameter\n",
    "output_size = 1\n",
    "model = GCN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the model\n",
    "auc_callback = AUCCallback()\n",
    "csv_logger = CSVLogger('lightning_logs', name='gcn_model')  # Initialize CSVLogger\n",
    "trainer = Trainer(max_epochs=10, accelerator=\"gpu\", callbacks=[auc_callback], logger=csv_logger)  # Add logger to Trainer\n",
    "trainer.fit(model, train_pyg_loader, valid_pyg_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'lightning_logs/gcn_model'\n",
    "latest_dir = find_latest_directory(base_path)\n",
    "plot_metrics(latest_dir, 'GCN Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test graph attention network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATv2Conv\n",
    "\n",
    "class GAT(BaseLitModel):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GAT, self).__init__(lr=1e-3, weight_decay=0, lr_decay=0.90)\n",
    "        self.node_network_1 = make_mlp(input_size, [hidden_size]*2, layer_norm=True, hidden_activation=\"ReLU\")\n",
    "        self.conv = GATv2Conv(hidden_size, hidden_size, heads=1)\n",
    "        self.node_network_2 = make_mlp(hidden_size, [hidden_size]*2, layer_norm=True, hidden_activation=\"ReLU\")\n",
    "        self.output_network = make_mlp(hidden_size, [output_size], layer_norm=True, hidden_activation=\"ReLU\", output_activation=\"Sigmoid\")\n",
    "        self.aggregation = aggr.MeanAggregation()\n",
    "\n",
    "    def forward(self, graph):\n",
    "        x = self.node_network_1(graph.x.float())\n",
    "        x = self.conv(x, graph.edge_index)\n",
    "        x = self.node_network_2(x)\n",
    "        x = self.aggregation(x, graph.batch)\n",
    "        return self.output_network(x)\n",
    "\n",
    "    def apply_model(self, batch):\n",
    "        return self(batch), batch.y\n",
    "\n",
    "# Initialize the model\n",
    "input_size = train_pyg_loader.dataset[0].x.shape[1]\n",
    "hidden_size = 64  # You can tune this parameter\n",
    "output_size = 1\n",
    "model = GAT(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the model\n",
    "auc_callback = AUCCallback()\n",
    "csv_logger = CSVLogger('lightning_logs', name='gat_model')  # Initialize CSVLogger\n",
    "trainer = Trainer(max_epochs=10, accelerator=\"gpu\", callbacks=[auc_callback], logger=csv_logger)  # Add logger to Trainer\n",
    "trainer.fit(model, train_pyg_loader, valid_pyg_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'lightning_logs/gat_model'\n",
    "latest_dir = find_latest_directory(base_path)\n",
    "plot_metrics(latest_dir, 'GAT Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
