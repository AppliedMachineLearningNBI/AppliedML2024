{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN and Natural Language Processing\n",
    "\n",
    "In this exercise we will try to classify IMDB dataset: Given the text of a review, can you predict if the review was positive or negative?\n",
    "\n",
    "Before doing this exercise, you might want to become more familier with LSTMs by considering the example FlightPassengerPredictions.\n",
    "\n",
    "The data for this exercise can be found here:\n",
    "https://sid.erda.dk/share_redirect/encok5nw3y\n",
    "\n",
    "***\n",
    "\n",
    "Author: Julius Kirkegaard and Troels C. Petersen<br>\n",
    "Date: 5th of May 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from torch import nn\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_data = 10000  # limit the amount of data for speed, change as you please\n",
    "\n",
    "def remove_special_symbols(string):\n",
    "    return ''.join(s for s in string if ord(s)>96 and ord(s)<123 or s == ' ')\n",
    "\n",
    "with open('train.json') as f:\n",
    "    train_text, train_labels = json.load(f)\n",
    "    idxs = np.random.permutation(len(train_text))\n",
    "    train_text = [remove_special_symbols(train_text[i]) for i in idxs[:limit_data]]\n",
    "    train_labels = [train_labels[i] for i in idxs[:limit_data]]\n",
    "    \n",
    "with open('test.json') as f:\n",
    "    test_text, test_labels = json.load(f)\n",
    "    idxs = np.random.permutation(len(test_text))   \n",
    "    test_text = [remove_special_symbols(test_text[i]) for i in idxs[:limit_data]]\n",
    "    test_labels = [test_labels[i] for i in idxs[:limit_data]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the data... Here is a negative review (label = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_text[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a positive one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_text[3])\n",
    "print(train_labels[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at all the words we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = list(chain(*[x.lower().split() for x in train_text]))\n",
    "print('total number of unique words =', len(set(all_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of words... of course, we could clean the data even more if we wanted to. But we won't...\n",
    "(for instance, there are probably many misspelled words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = 25000   # let's make a model that only understand 25000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, count = np.unique(all_words, return_counts=True)\n",
    "idxs = np.argsort(count)[-n_words:]\n",
    "vocab = ['<UNK>'] + list(words[idxs][::-1])\n",
    "print(vocab[:5], '...', vocab[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very surprisingly, the most commonly used word is _the_. The 25000th most used word is _chimneys_. We have added a special word `<UNK>` which we will used to mark words outside our vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now turn a sentence into a sequence of integers that correspond to the position in the vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_d = {vocab[i]: i for i in range(len(vocab))}  # for quick look-up\n",
    "def sentence_to_integer_sequence(s):\n",
    "    return torch.tensor([vocab_d[x] if x in vocab_d else 0 for x in s.split()], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_to_integer_sequence(\"i really liked the movie xenopus51\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representing high dimensional spaces with a simpler (learnable) embedding:\n",
    "\n",
    "We are now representing words in a \"25000\"-dimensional space: we have a unique integer for each word we can represent. To reduce this complexity, we instead intend to represent each word as a 50-dimensional real vector. Pytorch to the rescue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(len(vocab), 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Embedding` assigns are random, **but trainable** vector to each word. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding(sentence_to_integer_sequence(\"movie\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The special `<UNK>` word, which signifies unknown we can choose to zero out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.weight.data[0, :] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how we represent a sentence then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding(sentence_to_integer_sequence(\"i really liked the movie xenopus51\")).shape)\n",
    "print(embedding(sentence_to_integer_sequence(\"i really liked the movie xenopus51\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, the sentence bascially becomes a 6x50 pixel image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use `nn.LSTM` after an embedding to define a neural network for sentenes.\n",
    "\n",
    "This network will have a _lot_ of parameters. For each word, 50 parameters needs to be trained, and then comes the LSTM on top of that.\n",
    "This is the reason that _transfer learning_ is so important in natural language processing (NLP).\n",
    "\n",
    "Perhaps the simplest form of transfer learning is to use a pretrained embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('glove.6B.50d.pkl', 'rb') as f:\n",
    "    glove = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove['movie']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These pretrained word embeddings will have a good structure to them. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Distance b/w queen and prince =', np.linalg.norm(glove['queen'] - glove['prince']))\n",
    "print('Distance b/w movie and prince =', np.linalg.norm(glove['movie'] - glove['prince']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even sometimes get away with doing algebra with these vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queenlike = glove['king'] - glove['man'] + glove['woman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Distance b/w queen and algebraic queen =', np.linalg.norm(glove['queen'] - queenlike))\n",
    "print('Distance b/w queen and king =', np.linalg.norm(glove['queen'] - glove['king']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fill out our embedding layer using these pretrained vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled = 0\n",
    "not_found = []\n",
    "for i, w in enumerate(vocab):\n",
    "    if w in glove:\n",
    "        embedding.weight.data[i, :] = torch.tensor(glove[w], dtype=torch.float)\n",
    "        filled += 1\n",
    "    else:\n",
    "        not_found.append(w)\n",
    "print(f'Of the words in the vocab, {100 * filled / (len(vocab) - 1)} % were updated using Glove vectors')\n",
    "print('Examples of words not found :', not_found[1:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the words we do not find are due to misspellings. You now have three choices before you continue with the exercise:\n",
    "\n",
    "(1) Use _hunspell_ or similar to fix misspelled words\n",
    "\n",
    "(2) Change the vocabulary (`vocab`) to be based on words that are both frequent in the text and have glove vectors\n",
    "\n",
    "(3) Ignore the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, with Glove vectors we do not need to train the embedding. We can consider it fixed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a neural network using `nn.LSTM` layers to classify the IMDB reviews.\n",
    "\n",
    "Ideas:\n",
    "\n",
    " - The sentences can be quite long, so you might want to limit them to, say, maximum 100 words\n",
    " - `nn.LSTM` can take batched input, but be careful with `batch_first=True/False`.\n",
    " - If batched input are used, they normally have to be equal in size. You can put sentences to always be 100 words long, by adding `<UNK>` words to short sentences.\n",
    " - Alternatively, `nn.LSTM` can also accept batched, variable-length input using `torch.nn.utils.rnn.pack_sequence`.\n",
    " - (a finaly, albeit slow, alternative is to simple run the LSTM on un-batched input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often in NLP, a lot of unlabelled text is available. We can use this to pretrain the model, before fine-tuning to the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unlabelled.json') as f:\n",
    "    text = json.load(f)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to pretrain is to train a _language model_. This is a model that tries to predict the next word in a sentence. For instance, given:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(text[2].split()[:28])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can you guess the next word?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a language model, we consider the above the input, and the output is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[2].split()[28]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input we encode using the embedding, while the output is a probability map over words.\n",
    "In other words, the last layer is something like `nn.Linear(..., len(vocab))`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a language model.\n",
    "\n",
    "After you have trained the model, try to make to complete sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "### Exercise 3 (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discard the last layer of the now trained language model and use it to train on the original IMDB-problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
