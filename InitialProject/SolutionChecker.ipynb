{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AppliedML Initial Project solution file reader\n",
    "\n",
    "This notebook is used for reading solutions to the initial project and checking that they are valid.\n",
    "\n",
    "Note: It will only print the first 5 error messages of each check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining the folder holding the solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'solutions'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we read all the files in the folder, which correspond to the format, and verify the prediction/variablelist pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def init_entry():\n",
    "    tmp = {}\n",
    "    tmp['Classification'] = {}\n",
    "    tmp['Regression'] = {}\n",
    "    tmp['Clustering'] = {}\n",
    "    return tmp\n",
    "\n",
    "def read_filenames(directory):\n",
    "    tmp = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        full_path = f'{directory}/{filename}'\n",
    "        if not os.path.isfile(full_path) or not filename.endswith('.csv'):\n",
    "            continue\n",
    "        splitted = filename.split('_')\n",
    "        \n",
    "        project_part = splitted[0]\n",
    "        student_name = splitted[1]\n",
    "        is_varlist = splitted[-1].lower() == 'variablelist.csv'\n",
    "        implementation = splitted[-2] if is_varlist else splitted[-1].split('.csv')[0]\n",
    "        \n",
    "        if student_name not in tmp:\n",
    "            tmp[student_name] = init_entry()\n",
    "        if implementation not in tmp[student_name][project_part]:\n",
    "            tmp[student_name][project_part][implementation] = {}\n",
    "        \n",
    "        if is_varlist:\n",
    "            tmp[student_name][project_part][implementation]['vars'] = full_path\n",
    "        else:\n",
    "            tmp[student_name][project_part][implementation]['preds'] = full_path\n",
    "    return tmp\n",
    "\n",
    "all_errors = 0\n",
    "errors = 0\n",
    "def write_error(msg, cap=5):\n",
    "    global errors\n",
    "    if errors < cap:\n",
    "        print (msg)\n",
    "    errors += 1\n",
    "\n",
    "names = read_filenames(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can print the structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThomasSpieksma:\n",
      "    Classification:\n",
      "        XGBoost:\n",
      "            preds: solutions/Classification_ThomasSpieksma_XGBoost.csv\n",
      "            vars:  solutions/Classification_ThomasSpieksma_XGBoost_VariableList.csv\n",
      "    Regression:\n",
      "        TensorFlow:\n",
      "            preds: solutions/Regression_ThomasSpieksma_TensorFlow.csv\n",
      "            vars:  solutions/Regression_ThomasSpieksma_TensorFlow_VariableList.csv\n",
      "    Clustering:\n",
      "        KMeans:\n",
      "            preds: solutions/Clustering_ThomasSpieksma_KMeans.csv\n",
      "            vars:  solutions/Clustering_ThomasSpieksma_KMeans_VariableList.csv\n",
      "Files read succesfully\n"
     ]
    }
   ],
   "source": [
    "all_errors += errors\n",
    "errors = 0\n",
    "\n",
    "for name, parts in names.items():\n",
    "    print (f'{name}:')\n",
    "    for part, implementations in parts.items():\n",
    "        print (f'    {part}:')\n",
    "        if len(implementations) == 0:\n",
    "            write_error(f'        {part} does not have any files')\n",
    "        else:\n",
    "            for implementation, files in implementations.items():\n",
    "                if ('vars' not in files) and ('preds' not in files):\n",
    "                    write_error(f'            {implementation} does not have a full prediction/variablelist set')\n",
    "                else:\n",
    "                    print (f'        {implementation}:')\n",
    "                    print (f'            preds: {files[\"preds\"]}')\n",
    "                    print (f'            vars:  {files[\"vars\"]}')\n",
    "\n",
    "if errors == 0:\n",
    "    print ('Files read succesfully')\n",
    "else:\n",
    "    print (f'Reading files gave {errors} errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we verify the VariableList files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables parsed without error\n"
     ]
    }
   ],
   "source": [
    "all_variables = [\n",
    "    \"averageInteractionsPerCrossing\",\"p_Rhad1\",\"p_Rhad\",\"p_f3\",\"p_weta2\",\"p_Rphi\",\"p_Reta\",\"p_Eratio\",\"p_f1\",\"p_TRTPID\",\"p_numberOfInnermostPixelHits\",\"p_numberOfPixelHits\",\"p_numberOfSCTHits\",\"p_numberOfTRTHits\",\"p_TRTTrackOccupancy\",\"p_numberOfTRTXenonHits\",\"p_z0\",\"p_d0\",\"p_sigmad0\",\"p_dPOverP\",\"p_deltaEta1\",\"p_deltaPhiRescaled2\",\"p_etcone20\",\"p_etcone30\",\"p_etcone40\",\"p_ptcone20\",\"p_ptcone30\",\"p_ptcone40\",\"p_ptPU30\",\"p_vertex\",\"pX_E7x7_Lr2\",\"pX_E7x7_Lr3\",\"pX_E_Lr0_HiG\",\"pX_E_Lr0_MedG\",\"pX_E_Lr1_HiG\",\"pX_E_Lr1_LowG\",\"pX_E_Lr1_MedG\",\"pX_E_Lr2_HiG\",\"pX_E_Lr2_LowG\",\"pX_E_Lr2_MedG\",\"pX_E_Lr3_HiG\",\"pX_E_Lr3_MedG\",\"pX_MultiLepton\",\"pX_OQ\",\"pX_ambiguityType\",\"pX_asy1\",\"pX_author\",\"pX_barys1\",\"pX_core57cellsEnergyCorrection\",\"pX_deltaEta0\",\"pX_deltaEta1\",\"pX_deltaEta2\",\"pX_deltaEta3\",\"pX_deltaPhi0\",\"pX_deltaPhi1\",\"pX_deltaPhi2\",\"pX_deltaPhi3\",\"pX_deltaPhiFromLastMeasurement\",\"pX_deltaPhiRescaled0\",\"pX_deltaPhiRescaled1\",\"pX_deltaPhiRescaled3\",\"pX_e1152\",\"pX_e132\",\"pX_e235\",\"pX_e255\",\"pX_e2ts1\",\"pX_ecore\",\"pX_emins1\",\"pX_etcone20\",\"pX_etcone30\",\"pX_etcone40\",\"pX_f1core\",\"pX_f3core\",\"pX_maxEcell_energy\",\"pX_maxEcell_gain\",\"pX_maxEcell_time\",\"pX_maxEcell_x\",\"pX_maxEcell_y\",\"pX_maxEcell_z\",\"pX_nCells_Lr0_HiG\",\"pX_nCells_Lr0_MedG\",\"pX_nCells_Lr1_HiG\",\"pX_nCells_Lr1_LowG\",\"pX_nCells_Lr1_MedG\",\"pX_nCells_Lr2_HiG\",\"pX_nCells_Lr2_LowG\",\"pX_nCells_Lr2_MedG\",\"pX_nCells_Lr3_HiG\",\"pX_nCells_Lr3_MedG\",\"pX_neflowisol20\",\"pX_neflowisol30\",\"pX_neflowisol40\",\"pX_neflowisolcoreConeEnergyCorrection\",\"pX_pos\",\"pX_pos7\",\"pX_poscs1\",\"pX_poscs2\",\"pX_ptcone20\",\"pX_ptcone30\",\"pX_ptcone40\",\"pX_ptconecoreTrackPtrCorrection\",\"pX_ptvarcone20\",\"pX_ptvarcone30\",\"pX_ptvarcone40\",\"pX_r33over37allcalo\",\"pX_topoetcone20\",\"pX_topoetcone20ptCorrection\",\"pX_topoetcone30\",\"pX_topoetcone30ptCorrection\",\"pX_topoetcone40\",\"pX_topoetcone40ptCorrection\",\"pX_topoetconecoreConeEnergyCorrection\",\"pX_weta1\",\"pX_widths1\",\"pX_wtots1\",\"pX_e233\",\"pX_e237\",\"pX_e2tsts1\",\"pX_ehad1\",\"pX_emaxs1\",\"pX_fracs1\",\"pX_DeltaE\",\"pX_E3x5_Lr0\",\"pX_E3x5_Lr1\",\"pX_E3x5_Lr2\",\"pX_E3x5_Lr3\",\"pX_E5x7_Lr0\",\"pX_E5x7_Lr1\",\"pX_E5x7_Lr2\",\"pX_E5x7_Lr3\",\"pX_E7x11_Lr0\",\"pX_E7x11_Lr1\",\"pX_E7x11_Lr2\",\"pX_E7x11_Lr3\",\"pX_E7x7_Lr0\",\"pX_E7x7_Lr1\",\"p_pt_track\",\"p_eta\",\"p_phi\",\"p_charge\"\n",
    "]\n",
    "max_variables = {\n",
    "    'Classification': 20,\n",
    "    'Regression': 25,\n",
    "    'Clustering':  10,\n",
    "}\n",
    "\n",
    "all_errors += errors\n",
    "errors = 0\n",
    "for student_name, parts in names.items():\n",
    "    for part, implementations in parts.items():\n",
    "        for implementation, files in implementations.items():\n",
    "            file = files['vars']\n",
    "            count = 0\n",
    "            with open(file, 'r') as f:\n",
    "                for line in f:\n",
    "                    var_name = line.rstrip()\n",
    "                    if var_name not in all_variables:\n",
    "                        write_error(f'Variable {var_name} not in the given variable list {file}')\n",
    "                    else:\n",
    "                        count += 1\n",
    "            if count > max_variables[part]:\n",
    "                write_error(f'Used too many variables ({count}/{max_variables[part]}) for {part}: {file}')\n",
    "                    \n",
    "if errors == 0:\n",
    "    print ('Variables parsed without error')\n",
    "else:\n",
    "    print (f'Variables had {errors} errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can verify than the solution files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solutions parsed without error\n"
     ]
    }
   ],
   "source": [
    "test_entries_class = 60000\n",
    "test_entries_regre = 40000\n",
    "test_entries_clust = 20000\n",
    "\n",
    "prediction_range = {\n",
    "    'Classification': (0.0, 1.0),\n",
    "    'Regression': (-float('inf'), float('inf')),\n",
    "    'Clustering': (-float('inf'), float('inf')),\n",
    "}\n",
    "\n",
    "all_errors += errors\n",
    "errors = 0\n",
    "for student_name, parts in names.items():\n",
    "    for part, implementations in parts.items():\n",
    "        for implementation, files in implementations.items():\n",
    "            file = files['preds']\n",
    "            with open(file, 'r') as f:\n",
    "                lines = [line for line in f]\n",
    "            for i in range(len(lines)):\n",
    "                if ',' in lines[i]:\n",
    "                    index, value = lines[i].lstrip().rstrip().split(',')\n",
    "                    try:\n",
    "                        if int(index) != i:\n",
    "                            write_error(f'Index at line {i+1} does not have correct index: {index}')\n",
    "                    except ValueError:\n",
    "                        write_error(f'Unable to cast the index to an integer: {index} in {file}')\n",
    "                else:\n",
    "                    value = lines[i].lstrip().rstrip()\n",
    "                value = float(value)\n",
    "                if part == 'Clustering':\n",
    "                    if value.is_integer():\n",
    "                        value = int(value)\n",
    "                    else:\n",
    "                        write_error(f'Clustering value at {i} is not an integer: {value} in {file}')\n",
    "                        continue\n",
    "                mi, ma = prediction_range[part]\n",
    "                if not (value >= mi and value <= ma):\n",
    "                    write_error(f'Value at {i} is not in the permitted range of ({mi},{ma}): {value} in {file}')\n",
    "            if part == 'Classification':\n",
    "                if len(lines) != test_entries_class:\n",
    "                    write_error(f'Not correct number of predictions for classification. Got {len(lines)}, expected {test_entries_class}')\n",
    "            if part == 'Regression':\n",
    "                if len(lines) != test_entries_regre:\n",
    "                    write_error(f'Not correct number of predictions for regression. Got {len(lines)}, expected {test_entries_regre}')\n",
    "            if part == 'Clustering':\n",
    "                if len(lines) != test_entries_clust:\n",
    "                    write_error(f'Not correct number of predictions for clustering. Got {len(lines)}, expected {test_entries_clust}')\n",
    "                \n",
    "if errors == 0:\n",
    "    print ('Solutions parsed without error')\n",
    "else:\n",
    "    print (f'Solutions had {errors} errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we check if all of the steps completed without error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All parts of this submission had no errors\n"
     ]
    }
   ],
   "source": [
    "if all_errors == 0:\n",
    "    print ('All parts of this submission had no errors')\n",
    "else:\n",
    "    print (f'This submission had {all_errors} errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "\n",
    "### Note in program format:\n",
    "\n",
    "In case you want to export the notebook to a \"normal\" python file (.py), you can uncomment the command below (but keep the \"`!`\") and run the cell. This exports the notebook to a Python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook SolutionReader.ipynb to script\n",
      "[NbConvertApp] Writing 8718 bytes to SolutionReader.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script SolutionReader.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5f0708b43c0165d619d9ef529ccea0495844a56cda68c478552c0aefa21ef634"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
