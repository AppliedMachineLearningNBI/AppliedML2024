{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction 0\n",
    "\n",
    "Practice applying and using dimensionality reduction for analysing datasets with Principal Component Analysis (`PCA` & `Kernel PCA`). Exercise is to be continued with more advanced algorithms: t-SNE and UMAP.\n",
    "\n",
    "---\n",
    "\n",
    "### Data\n",
    "\n",
    "We will start by using the \"Wine data set\", which is a clean multi-variate set used to demonstrate classification algorithms. There are 13 features:\n",
    "\n",
    "`1.  Alcohol`\\\n",
    "`2.  Malic acid`\\\n",
    "`3.  Ash`\\\n",
    "`4.  Alcalinity of ash`\\\n",
    "`5.  Magnesium`\\\n",
    "`6.  Total phenols`\\\n",
    "`7.  Flavanoids`\\\n",
    "`8.  Nonflavanoid phenols`\\\n",
    "`9.  Proanthocyanins`\\\n",
    "`10. Color intensity`\\\n",
    "`11. Hue`\\\n",
    "`12. OD280/OD315 of diluted wines`\\\n",
    "`13. Proline`\n",
    "\n",
    "The data set is small and contains three classes of objects, each with a fairly well-defined feature space. These features have largely linear relationships, which makes it well-suited for demonstrating how PCA works.\n",
    "\n",
    "Source: https://archive.ics.uci.edu/ml/datasets/wine.\n",
    "\n",
    "---\n",
    "\n",
    "* Authors:  Vadim Rusakov, Troels Petersen\n",
    "* Email:    petersen@nbi.dk\n",
    "* Date:     15th of May 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.decomposition import PCA, KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a quick function for plotting our PCA components\n",
    "def plot_pca(y_pcs, y):\n",
    "    #=== plot PCA results\n",
    "    fig, ax = plt.subplots(1, figsize=(5, 5), dpi=100)\n",
    "    #ax.set_xlim(np.percentile(y_pcs[:,0], 99), np.percentile(y_pcs[:,0], 1))\n",
    "    #ax.set_ylim(np.percentile(y_pcs[:,1], 99), np.percentile(y_pcs[:,1], 1))\n",
    "    ax.set_xlabel('Component 1')\n",
    "    ax.set_ylabel('Component 2')\n",
    "\n",
    "    # locate the points of each type in the original data\n",
    "    # and paint them over the transformed data\n",
    "    is_type1 = (y == 0)\n",
    "    is_type2 = (y == 1)\n",
    "    is_type3 = (y == 2)\n",
    "    ax.scatter(y_pcs[is_type1, 0], y_pcs[is_type1, 1], \n",
    "               c='y', marker='s', label='Type 1')\n",
    "    ax.scatter(y_pcs[is_type2, 0], y_pcs[is_type2, 1], \n",
    "               c='b', marker='o', label='Type 2')\n",
    "    ax.scatter(y_pcs[is_type3, 0], y_pcs[is_type3, 1], \n",
    "               c='g', marker='^', label='Type 3')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the data, split it into features `X` and labels `y`. Then apply PCA to it and plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the wine dataset\n",
    "data = load_wine()\n",
    "X = data['data']\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA (Raw data):\n",
    "\n",
    "Let us apply the PCA without doing anything to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, svd_solver='full') # get a pca object of class PCA()\n",
    "y_pcs = pca.fit_transform(X) # train pca object\n",
    "\n",
    "# plot PCA results\n",
    "plot_pca(y_pcs, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA (normalized data):\n",
    "\n",
    "Ok, the three groups do not appear to be distinct. However, we would expect for these classes to have distinct principal components. So something does not quite work. Remember to check the distributions of variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(8, 4), dpi=100)\n",
    "ax.set_xlabel('log10( Variable values )')\n",
    "ax.set_ylabel('Number')\n",
    "ax.set_title('Raw data')\n",
    "xbins = np.arange(-1, 3.5, 0.1)\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    ax.hist(np.log10(X[:, i]), bins=xbins, histtype='step', label=f'Var {i}')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see that the distributions of variables are normalized very differently. Let's fix it and try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize X\n",
    "transform = preprocessing.Normalizer(norm='l2')\n",
    "X_norm = transform.fit_transform(X)\n",
    "\n",
    "# transform X_norm\n",
    "y_pcs = pca.fit_transform(X_norm) # train pca object\n",
    "\n",
    "# plot new PCA results\n",
    "plot_pca(y_pcs, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still does not work, though Type 1 became a bit more distinct. Let's see those new distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(8, 4), dpi=100)\n",
    "ax.set_xlabel('log10( Variable values )')\n",
    "ax.set_ylabel('Number')\n",
    "ax.set_title('Normalized data')\n",
    "xbins = np.arange(-4, 0, 0.1)\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    x_pos = X_norm[:, i][X_norm[:, i] > 0.0]\n",
    "    ax.hist(np.log10(x_pos), bins=xbins, histtype='step', label=f'Var {i}')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA (standardized data):\n",
    "\n",
    "They are centered similarly, but seem to have different scales. Can we do better by scaling them similarly in addition to normalizing (i.e., standardizing them)? Looks much better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize X\n",
    "transform = preprocessing.StandardScaler()\n",
    "X_std = transform.fit_transform(X)\n",
    "\n",
    "# Transform X_std\n",
    "y_pcs = pca.fit_transform(X_std) # train pca object\n",
    "\n",
    "# Plot new PCA results\n",
    "plot_pca(y_pcs, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See, the distributions are much more separated now. You can play around with other data transformers too. You may want to think about whether you have any outliers and how to deal with them. Also, sometimes variables can have different underlying distributions and therefore need to be transformed to the same (ideally, symmetric and standardized) distribution. Here is the link:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(8, 4), dpi=100)\n",
    "ax.set_xlabel('log10( Variable values )')\n",
    "ax.set_ylabel('Number')\n",
    "ax.set_title('Normalized data')\n",
    "xbins = np.arange(-3.2, 1.0, 0.2)\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    x_pos = X_std[:, i][X_std[:, i] > 0.0]\n",
    "    ax.hist(np.log10(x_pos), bins=xbins, histtype='step', label=f'Var {i}')\n",
    "\n",
    "ax.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel PCA:\n",
    "\n",
    "Finally, we can comoute the principal components using a non-linear kernel, which can slightly improve the classification in this case. \n",
    "\n",
    "`KernelPCA` is a variant of the PCA, which can use a range of kernels for non-linear operations. I.e., this extension gives flexibility in separating the data that are not linearly-separable. Make sure to try different kernels for reducing the dimensionality. See documentation for `KernelPCA` in **sklearn**.\n",
    "\n",
    "For Kernel PCA see the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize X\n",
    "transform = preprocessing.StandardScaler()\n",
    "X_std = transform.fit_transform(X)\n",
    "\n",
    "# transform X_norm\n",
    "kpca = KernelPCA(n_components=2, kernel='rbf')\n",
    "y_pcs = kpca.fit_transform(X_std)\n",
    "\n",
    "# plot new PCA results\n",
    "plot_pca(y_pcs, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise:\n",
    "\n",
    "The above code was the simply introduction to dimensionality reduction. Now expand in methods (t-SNE and UMAP) and dataset:\n",
    "1. Try the t-SNE and UMAP methods on the wine set (if nothing else, then just to see that they work here).\n",
    "2. Now use the same methods on the \"Galaxies\" data set (cosmos2015.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning points:\n",
    "\n",
    "1. You should first of all know that these methods exists, that they are unsupervised, and that their foremost capability is to reduce high dimensional data (where we can't \"see\" the data) to lower (2-3) dimensional data, where we can \"see\" it in a plot.\n",
    "2. The PCA is only linear but very fast, while the other methods are non-linear and powerful but slow. For this reason, it makes sense to use both the PCA and e.g. UMAP.\n",
    "3. Only use the t-SNE and UMAP methods for up to $10^4$-$10^5$ events, possibly selected at random from a larger dataset.\n",
    "4. Dimensionality reduction methods can be used for many things, anomaly detection among them. One can simply run it on \"unknown\" data, and get an idea of the data structure fast. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c5875523c0f295e9a7a392db41a9a428ae2dda35a82a766a28c29c1f8950fbf4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
