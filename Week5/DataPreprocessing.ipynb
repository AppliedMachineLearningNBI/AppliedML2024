{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing:\n",
    "\n",
    "### Problem:\n",
    "Until know, we have considered data, which is nice and clean. No NaN-values have been present, as have there been no serious outliers. In this exercise we will consider especially the first problem (non-values!), and how to deal with it.\n",
    "\n",
    "We will use the Aleph b-quark jet data (50000 events), which exists in two version:\n",
    "1. Normal (which means \"perfect\")\n",
    "2. Flawed (where NaN-values have been introduced)\n",
    "\n",
    "We want you to preprocess the flawed data to a point where you can train a Neural Net on it (further specified below).\n",
    "\n",
    "\n",
    "### Data:\n",
    "The input variables (X) are (where Aleph uses only **the first six**):\n",
    "* **prob_b**: Probability of being a b-jet from the pointing of the tracks to the vertex.\n",
    "* **spheri**: Sphericity of the event, i.e. how spherical it is.\n",
    "* **pt2rel**: The transverse momentum squared of the tracks relative to the jet axis, i.e. width of the jet.\n",
    "* **multip**: Multiplicity of the jet (in a relative measure).\n",
    "* **bqvjet**: b-quark vertex of the jet, i.e. the probability of a detached vertex.\n",
    "* **ptlrel**: Transverse momentum (in GeV) of possible lepton with respect to jet axis (about 0 if no leptons).\n",
    "* energy: Measured energy of the jet in GeV. Should be 45 GeV, but fluctuates.\n",
    "* cTheta: cos(theta), i.e. the polar angle of the jet with respect to the beam axis. Note, that the detector works best in the central region (|cTheta| small) and less well in the forward regions.\n",
    "* phi:    The azimuth angle of the jet. As the detector is uniform in phi, this should not matter (much).\n",
    "\n",
    "The target variable (Y) is:\n",
    "* isb:    1 if it is from a b-quark and 0, if it is not.\n",
    "\n",
    "Finally, those before you (the Aleph collaboration in the mid 90'ies) produced a Neural Net (6 input variables, two hidden layers with 10 neurons in each, and 1 output varible) based classification variable, which you can compare to (and compete with?):\n",
    "* nnbjet: Value of original Aleph b-jet tagging algorithm, using only the last six variables (for reference).\n",
    "\n",
    "---\n",
    "\n",
    "* Author: Troels C. Petersen (NBI)\n",
    "* Email:  petersen@nbi.dk\n",
    "* Date:   21st of May 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division   # Ensures Python3 printing & division standard\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import LogNorm\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd \n",
    "from pandas import Series, DataFrame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible other packages to consider:\n",
    "cornerplot, seaplot, sklearn.decomposition(PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random\n",
    "r.seed(42)\n",
    "\n",
    "SavePlots = False\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('AlephBtag_MC_train_Nev50000_flawed.csv')  #Read the data\n",
    "variables = data.columns   #Get columns names\n",
    "\n",
    "# Sometimes when the csv file had been created, the columns get saved as \"object\" type, we need to convert into float\n",
    "# This is the case for the flawed datset\n",
    "data['prob_b'] = data['prob_b'].astype(float)\n",
    "data['spheri'] = data['spheri'].astype(float)\n",
    "data['pt2rel'] = data['pt2rel'].astype(float)\n",
    "data['bqvjet'] = data['bqvjet'].astype(float)\n",
    "data['ptlrel'] = data['ptlrel'].astype(float)\n",
    "data['energy'] = data['energy'].astype(float)\n",
    "data['cTheta'] = data['cTheta'].astype(float)\n",
    "data['phi']    = data['phi'].astype(float)\n",
    "data['multip'] = data['multip'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs: 23461\n"
     ]
    }
   ],
   "source": [
    "# Total number of NaNs:\n",
    "nan_count = data.isna().sum().sum()\n",
    "print('Number of NaNs:', nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 0, Missing: 395 (0.79%)\n",
      "> 1, Missing: 416 (0.83%)\n",
      "> 2, Missing: 422 (0.84%)\n",
      "> 3, Missing: 407 (0.81%)\n",
      "> 4, Missing: 385 (0.77%)\n",
      "> 5, Missing: 413 (0.83%)\n",
      "> 6, Missing: 424 (0.85%)\n",
      "> 7, Missing: 411 (0.82%)\n",
      "> 8, Missing: 20188 (40.38%)\n",
      "> 9, Missing: 0 (0.00%)\n",
      "> 10, Missing: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Count number of missing values in each column\n",
    "total_rows = data.shape[0]\n",
    "perc_dict = {}\n",
    "for i in range(data.shape[1]):\n",
    "    n_miss = data.iloc[:, i].isnull().sum()\n",
    "    perc = (n_miss / total_rows) * 100\n",
    "    perc_dict[i] = perc\n",
    "    print('> %d, Missing: %d (%.2f%%)' % (i, n_miss, perc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-f7d148d1db99>, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-f7d148d1db99>\"\u001b[0;36m, line \u001b[0;32m38\u001b[0m\n\u001b[0;31m    plt.plot(fpr_dict[f'fpr_{i}'], tpr_dict[f'tpr_{i}'], label=f'x{i}: {column_name} {perc_dict[i]}% (AUC = {auc_dict[f'auc_{i}']:5.3f})')\u001b[0m\n\u001b[0m                                                                                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Evaluate:\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "input_variables = variables[(variables != 'nnbjet') & (variables != 'isb') &\n",
    "                            (variables != 'energy') & (variables != 'phi') & (variables != 'cTheta')]\n",
    "input_data = data[input_variables]\n",
    "truth_data = data['isb']\n",
    "benchmark_data = data['nnbjet']\n",
    "\n",
    "# Normalize the input data\n",
    "scaler = MinMaxScaler()\n",
    "normalized_input_data = pd.DataFrame(scaler.fit_transform(input_data), columns=input_variables)\n",
    "\n",
    "# Initialize dictionaries to store fpr and tpr values\n",
    "fpr_dict = {}\n",
    "tpr_dict = {}\n",
    "auc_dict = {}\n",
    "\n",
    "# Loop through each feature column (excluding the last column 'isb')\n",
    "for i in range(data.shape[1] - 2):\n",
    "    column_name = normalized_input_data.columns[i]\n",
    "    filtered_data = pd.concat([normalized_input_data[column_name], data['isb']], axis=1).dropna()\n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, _ = roc_curve(filtered_data['isb'],filtered_data[column_name])\n",
    "    auc_score = auc(fpr,tpr)  \n",
    "    # Store the results in the dictionaries\n",
    "    fpr_dict[f'fpr_{i}'] = fpr\n",
    "    tpr_dict[f'tpr_{i}'] = tpr\n",
    "    auc_dict[f'auc_{i}'] = auc_score\n",
    "\n",
    "# Let's plot the ROC curves for these results:\n",
    "fig = plt.figure(figsize = [10,10])\n",
    "plt.title('ROC from non-NaNs entries', size = 16)\n",
    "for i in range(data.shape[1] - 2):\n",
    "    column_name = normalized_input_data.columns[i]\n",
    "    plt.plot(fpr_dict[f'fpr_{i}'], tpr_dict[f'tpr_{i}'], label=f'x{i}: {column_name} {perc_dict[i]}% (AUC = {auc_dict[f'auc_{i}']:5.3f})')\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel('False Postive Rate', size=16)\n",
    "plt.ylabel('True Positive Rate', size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get portion of NaNs per event\n",
    "total_columns = data.shape[1]\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "missing_data_per_event = (data.apply(lambda row: row.isnull().sum(), axis=1))/ total_columns\n",
    "\n",
    "# Combine results into a new DataFrame\n",
    "result_df = pd.DataFrame({\n",
    "    'portion_NaNs': missing_data_per_event,\n",
    "})\n",
    "\n",
    "# Print or inspect the result\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "bins = np.linspace(0,2,102)\n",
    "axs.hist(result_df, histtype='step')\n",
    "axs.set_xlabel('$\\\\rho_{NaN}$')#\n",
    "axs.set_ylabel('Events')\n",
    "\n",
    "# axs.grid(color=\"grey\")\n",
    "axs.legend( loc=\"upper right\",fontsize=14)\n",
    "axs.set_xlim(0,1)\n",
    "axs.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggested problems:\n",
    "\n",
    "1. We have kindly provided you with a plot that shows you how strong the different input variables are and <b>what fraction of these are NaN-values</b>. Consider this plot, and discuss in your group how to interpret it, and how to use it! Based on this, would you consider excluding any of the input variables?\n",
    "\n",
    "2. We also kindly provide you with code for a plot, that shows <b>the fraction of NaNs in the entries</b>. Once again, consider this plot and discuss how to use it.\n",
    "\n",
    "3. Apply a BDT to both the \"normal\" and the (original) \"flawed\" datasets, and see to what extend the NaNs ruins the training by considering the performance in a \"normal\" test set. Any degredation?\n",
    "\n",
    "4. Apply an NN to both the \"normal\" and the preprocessed (repaired?) \"flawed\" datasets, and see to what extend the NaNs ruins the training by considering the performance in a \"normal\" test set. Any degredation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning points:\n",
    "\n",
    "From this exercise you should learn to inspect and \"repair\" your data. The exercise focuses on NaN-values. The typical pitfalls you want to get rid of are:\n",
    "1. NaN-values and \"Non-values\" (i.e. -9999)\n",
    "2. Wild outliers (i.e. values much outside the typical range)\n",
    "3. Shifts in distributions (i.e. part of data having a different mean/width/definition/etc)\n",
    "\n",
    "You should have learned how to find, evaluate, and eliminate NaN-values first column-wise (input variables) and then row-wise (entries). And you should have understood the concept of \"imputation\" (replacing missing data with actual values) and methods to do so (mean, median, and more advanced methods).\n",
    "\n",
    "Finally, you should have gained some experience with the impact of NaN-values on the performance of a subsequent ML analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
